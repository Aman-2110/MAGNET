{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "import csv\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/RCV1-V2/rcv1_X_train.pickle', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open('../Dataset/RCV1-V2/rcv1_y_train.pickle', 'rb') as f:\n",
    "    y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611354/611354 [00:54<00:00, 11266.08it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_count = [0] * 103\n",
    "\n",
    "for label in tqdm(y_train):\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 1:\n",
    "            labels_count[i] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.bar(np.arange(103), labels_count)\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(611354, 200) (611354, 103)\n",
      "(32177, 200) (32177, 103)\n",
      "(160883, 200) (160883, 103)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2idx, maxlength\n",
    "with open('RCV1-V2_model/maxlength.pkl', 'wb') as f:\n",
    "    pickle.dump(max_length, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sequence, labels):\n",
    "        self.sequence = sequence\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_in = torch.tensor(self.sequence[idx]).float()\n",
    "        output_label = torch.tensor(self.labels[idx]).float()\n",
    "        return sequence_in, output_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataset_train = CustomDataset(X_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_val = CustomDataset(X_val, y_val)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = CustomDataset(X_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MLTC(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, dropout=0.2):\n",
    "        super(MLP_MLTC, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "\n",
    "        output = self.fc1(sentence)\n",
    "        output = torch.relu(output)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        output = self.fc2(output)\n",
    "        output = torch.relu(output)\n",
    "        output = self.dropout2(output)\n",
    "        \n",
    "        output = self.fc3(output)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 103\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 200\n",
    "dropout = 0.2\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "model = MLP_MLTC(embedding_dim, num_classes, dropout)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_function, optimizer, epochs, device):\n",
    "\n",
    "    train_loss, val_loss = [], []\n",
    "    total_time = 0\n",
    "    min_average_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(\"Epoch : \", epoch + 1)\n",
    "\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        y_true_train, y_pred_train = [], []\n",
    "        for batch in tqdm(dataloader_train, desc='Training'):\n",
    "            sequence_in, output_label = batch\n",
    "            sequence_in, output_label = sequence_in.to(device), output_label.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            pred_batch = model(sequence_in)\n",
    "            loss = loss_function(pred_batch, output_label)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            y_true_train.extend(output_label.cpu().detach().numpy().tolist())\n",
    "            y_pred_train.extend(pred_batch.cpu().detach().numpy().tolist())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        model.eval()        \n",
    "        total_val_loss = 0\n",
    "        y_true_val, y_pred_val = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader_val, desc='Evaluate'):\n",
    "                sequence_in, output_label = batch\n",
    "                sequence_in, output_label = sequence_in.to(device), output_label.to(device)\n",
    "\n",
    "                pred_batch = model(sequence_in)\n",
    "                loss = loss_function(pred_batch, output_label)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                y_true_val.extend(output_label.cpu().detach().numpy().tolist())\n",
    "                y_pred_val.extend(pred_batch.cpu().detach().numpy().tolist())\n",
    "\n",
    "        y_true_train = np.array(y_true_train)\n",
    "        y_pred_train = np.array(y_pred_train)\n",
    "        y_true_val = np.array(y_true_val)\n",
    "        y_pred_val = np.array(y_pred_val)\n",
    "\n",
    "        y_pred_train = y_pred_train >= 0.5\n",
    "        y_pred_val = y_pred_val >= 0.5\n",
    "\n",
    "        train_f1_score_micro = metrics.f1_score(y_true_train, y_pred_train, average='micro')\n",
    "        val_f1_score_micro = metrics.f1_score(y_true_val, y_pred_val, average='micro')\n",
    "\n",
    "        average_train_loss = float(total_train_loss / len(dataloader_train))\n",
    "        average_val_loss = float(total_val_loss / len(dataloader_val))\n",
    "\n",
    "        print('Train F1 Score (Micro)', train_f1_score_micro)\n",
    "        print('Val F1 Score (Micro)', val_f1_score_micro)\n",
    "        print('Train loss', average_train_loss)\n",
    "        print('Val loss', average_val_loss)\n",
    "\n",
    "        train_loss.append(average_train_loss)\n",
    "        val_loss.append(average_val_loss)\n",
    "\n",
    "        if average_val_loss < min_average_loss:\n",
    "            min_average_loss = average_val_loss\n",
    "            torch.save(model, 'RCV1-V2_model/model.pth')\n",
    "            print(\"Best model saved\")\n",
    "            \n",
    "        print(\"------------------------------------------------\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        total_time += epoch_time\n",
    "\n",
    "    avg_epoch_time = total_time / epochs\n",
    "    return train_loss, val_loss, avg_epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [03:02<00:00, 52.23it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:02<00:00, 208.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.711706103938423\n",
      "Val F1 Score (Micro) 0.8137680813481644\n",
      "Train loss 0.047990333165875546\n",
      "Val loss 0.030168786200566983\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [01:50<00:00, 86.64it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:02<00:00, 212.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8070012763431613\n",
      "Val F1 Score (Micro) 0.8323448064718673\n",
      "Train loss 0.032236044814851685\n",
      "Val loss 0.02740019929426802\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [02:07<00:00, 75.10it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:02<00:00, 183.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8176328723198457\n",
      "Val F1 Score (Micro) 0.8385341522733831\n",
      "Train loss 0.030625738278982854\n",
      "Val loss 0.02644133876831406\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [02:11<00:00, 72.67it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:02<00:00, 204.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8224018579912588\n",
      "Val F1 Score (Micro) 0.84049152516153\n",
      "Train loss 0.029869343845531894\n",
      "Val loss 0.025941791314683542\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [02:04<00:00, 76.62it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:02<00:00, 211.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8243759073341481\n",
      "Val F1 Score (Micro) 0.8430399876882039\n",
      "Train loss 0.02950721516765363\n",
      "Val loss 0.02569242294648177\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [01:40<00:00, 95.12it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:39<00:00, 12.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8261908321200777\n",
      "Val F1 Score (Micro) 0.8438297894201476\n",
      "Train loss 0.029246181053760992\n",
      "Val loss 0.025529550181083365\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [01:49<00:00, 87.57it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:03<00:00, 155.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8272427227134908\n",
      "Val F1 Score (Micro) 0.8447991787501604\n",
      "Train loss 0.02906431544101823\n",
      "Val loss 0.025360902984280353\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [02:16<00:00, 70.13it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:02<00:00, 225.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8281200338768213\n",
      "Val F1 Score (Micro) 0.8445940044696755\n",
      "Train loss 0.02891180188754139\n",
      "Val loss 0.02526402560101589\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [01:39<00:00, 96.34it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:02<00:00, 216.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8287575006467647\n",
      "Val F1 Score (Micro) 0.845763033369733\n",
      "Train loss 0.028790830186275047\n",
      "Val loss 0.025245329368837312\n",
      "Best model saved\n",
      "------------------------------------------------\n",
      "Epoch :  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9553/9553 [02:20<00:00, 68.23it/s] \n",
      "Evaluate: 100%|██████████| 503/503 [00:02<00:00, 224.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score (Micro) 0.8292609496547222\n",
      "Val F1 Score (Micro) 0.8465004690647925\n",
      "Train loss 0.02870226881077723\n",
      "Val loss 0.025123247658282104\n",
      "Best model saved\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, avg_epoch_time = train_model(model, loss_function, optimizer, epochs, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scores(model, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader_test, desc='Evaluate'):\n",
    "            sequence_in, output_label = batch\n",
    "            sequence_in, output_label = sequence_in.to(device), output_label.to(device)\n",
    "\n",
    "            pred_batch = model(sequence_in)\n",
    "            pred_batch = (pred_batch > 0.5).float()\n",
    "\n",
    "            y_true.extend(output_label.cpu().detach().numpy().tolist())\n",
    "            y_pred.extend(pred_batch.cpu().detach().numpy().tolist())\n",
    "\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    f1_score_micro = metrics.f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    print('Test Accuracy', accuracy)\n",
    "    print('Test F1 Score (Micro)', f1_score_micro)\n",
    "\n",
    "    print('Classification report')\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 2514/2514 [00:11<00:00, 213.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.5985840641957199\n",
      "Test F1 Score (Micro) 0.8449079842683609\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.21      0.32      4838\n",
      "           1       0.81      0.61      0.70      2460\n",
      "           2       0.75      0.28      0.41      7672\n",
      "           3       0.76      0.31      0.44      1573\n",
      "           4       0.94      0.92      0.93     30397\n",
      "           5       0.95      0.88      0.91     16497\n",
      "           6       0.90      0.77      0.83      4692\n",
      "           7       0.88      0.80      0.84     14532\n",
      "           8       0.79      0.06      0.11       382\n",
      "           9       0.87      0.73      0.79      8390\n",
      "          10       0.85      0.66      0.74      3652\n",
      "          11       0.89      0.75      0.81      2301\n",
      "          12       0.75      0.54      0.63       519\n",
      "          13       0.94      0.90      0.92      1154\n",
      "          14       0.88      0.79      0.83     10604\n",
      "          15       0.83      0.76      0.79      8717\n",
      "          16       0.89      0.13      0.23       919\n",
      "          17       0.83      0.52      0.64      1485\n",
      "          18       0.83      0.31      0.46      5143\n",
      "          19       0.72      0.08      0.15      1214\n",
      "          20       0.72      0.16      0.26       510\n",
      "          21       0.79      0.46      0.58      6346\n",
      "          22       0.77      0.47      0.58      8074\n",
      "          23       0.88      0.02      0.03       886\n",
      "          24       0.69      0.08      0.14      1329\n",
      "          25       0.00      0.00      0.00       231\n",
      "          26       0.88      0.04      0.07       428\n",
      "          27       0.85      0.41      0.55      3051\n",
      "          28       0.80      0.48      0.60       223\n",
      "          29       0.81      0.27      0.41      1009\n",
      "          30       0.89      0.74      0.81      2290\n",
      "          31       0.88      0.79      0.84      2071\n",
      "          32       0.81      0.71      0.76      2308\n",
      "          33       0.95      0.95      0.95     76282\n",
      "          34       0.85      0.43      0.57      1742\n",
      "          35       0.77      0.49      0.60      5397\n",
      "          36       0.91      0.60      0.73       440\n",
      "          37       0.88      0.72      0.79      1314\n",
      "          38       0.85      0.68      0.75      1125\n",
      "          39       0.88      0.41      0.56       193\n",
      "          40       0.88      0.31      0.46       439\n",
      "          41       0.00      0.00      0.00        76\n",
      "          42       0.67      0.05      0.09        44\n",
      "          43       0.87      0.49      0.63       241\n",
      "          44       0.90      0.78      0.84      8729\n",
      "          45       0.82      0.66      0.73      3160\n",
      "          46       0.93      0.78      0.85      5575\n",
      "          47       0.82      0.58      0.68       478\n",
      "          48       0.79      0.64      0.71       335\n",
      "          49       0.00      0.00      0.00         6\n",
      "          50       0.00      0.00      0.00        16\n",
      "          51       0.85      0.75      0.80      3348\n",
      "          52       0.69      0.54      0.60       435\n",
      "          53       0.83      0.54      0.65      4276\n",
      "          54       0.69      0.18      0.29       573\n",
      "          55       0.81      0.55      0.65      2549\n",
      "          56       0.88      0.70      0.78       464\n",
      "          57       0.88      0.29      0.44        75\n",
      "          58       0.97      0.92      0.95      1086\n",
      "          59       0.90      0.81      0.85     24004\n",
      "          60       0.87      0.87      0.87      4115\n",
      "          61       0.48      0.04      0.08       676\n",
      "          62       0.33      0.00      0.00       407\n",
      "          63       0.76      0.49      0.59       463\n",
      "          64       0.82      0.83      0.82      1635\n",
      "          65       0.00      0.00      0.00       415\n",
      "          66       0.00      0.00      0.00        63\n",
      "          67       0.77      0.69      0.72       424\n",
      "          68       0.80      0.57      0.67       867\n",
      "          69       0.00      0.00      0.00         7\n",
      "          70       0.94      0.94      0.94     47756\n",
      "          71       0.81      0.74      0.77      6463\n",
      "          72       0.79      0.32      0.46      1714\n",
      "          73       0.83      0.68      0.74      7456\n",
      "          74       0.85      0.62      0.72      1713\n",
      "          75       0.69      0.14      0.23       748\n",
      "          76       0.63      0.08      0.15      1277\n",
      "          77       0.00      0.00      0.00        59\n",
      "          78       0.78      0.36      0.49      1256\n",
      "          79       0.86      0.75      0.80      3385\n",
      "          80       0.00      0.00      0.00         0\n",
      "          81       0.00      0.00      0.00       166\n",
      "          82       0.00      0.00      0.00       592\n",
      "          83       0.82      0.68      0.74     11247\n",
      "          84       0.71      0.14      0.23      1133\n",
      "          85       0.81      0.02      0.04       562\n",
      "          86       0.88      0.36      0.51       452\n",
      "          87       0.98      0.97      0.97      7166\n",
      "          88       0.75      0.02      0.04       141\n",
      "          89       0.85      0.66      0.74      6458\n",
      "          90       0.77      0.61      0.68      2248\n",
      "          91       0.81      0.68      0.74       748\n",
      "          92       0.00      0.00      0.00       389\n",
      "          93       0.94      0.91      0.92      9860\n",
      "          94       0.88      0.82      0.85      5304\n",
      "          95       0.92      0.89      0.90     10683\n",
      "          96       0.92      0.82      0.87      5582\n",
      "          97       0.89      0.85      0.87      5348\n",
      "          98       0.94      0.95      0.94     16980\n",
      "          99       0.92      0.96      0.94      9322\n",
      "         100       0.91      0.87      0.89      2473\n",
      "         101       0.95      0.86      0.90      4429\n",
      "         102       0.95      0.94      0.94     40994\n",
      "\n",
      "   micro avg       0.91      0.79      0.84    521475\n",
      "   macro avg       0.73      0.49      0.55    521475\n",
      "weighted avg       0.89      0.79      0.83    521475\n",
      " samples avg       0.92      0.83      0.85    521475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('RCV1-V2_model/model.pth')\n",
    "test_scores(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_MLTC(\n",
      "  (fc1): Linear(in_features=200, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=103, bias=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "The model has 40,679 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "\n",
    "print(model)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss [0.047990333165875546, 0.032236044814851685, 0.030625738278982854, 0.029869343845531894, 0.02950721516765363, 0.029246181053760992, 0.02906431544101823, 0.02891180188754139, 0.028790830186275047, 0.02870226881077723]\n",
      "Val loss [0.030168786200566983, 0.02740019929426802, 0.02644133876831406, 0.025941791314683542, 0.02569242294648177, 0.025529550181083365, 0.025360902984280353, 0.02526402560101589, 0.025245329368837312, 0.025123247658282104]\n"
     ]
    }
   ],
   "source": [
    "print('Train loss', train_loss)\n",
    "print('Val loss', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg epoch time 172.03 secs\n"
     ]
    }
   ],
   "source": [
    "print('Avg epoch time', round(avg_epoch_time, 2), 'secs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a755c3486a199bb280be05c1d30ef5249180d4344c9bdb63523ed58db47fcbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
